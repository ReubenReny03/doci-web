"use strict";(self.webpackChunkdocusaurus_yt_example=self.webpackChunkdocusaurus_yt_example||[]).push([[6136],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>d});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),s=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=s(e.components);return r.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,m=p(e,["components","mdxType","originalType","parentName"]),c=s(n),g=a,d=c["".concat(l,".").concat(g)]||c[g]||u[g]||o;return n?r.createElement(d,i(i({ref:t},m),{},{components:n})):r.createElement(d,i({ref:t},m))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=g;var p={};for(var l in t)hasOwnProperty.call(t,l)&&(p[l]=t[l]);p.originalType=e,p[c]="string"==typeof e?e:a,i[1]=p;for(var s=2;s<o;s++)i[s]=n[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}g.displayName="MDXCreateElement"},3894:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>p,toc:()=>s});var r=n(7462),a=(n(7294),n(3905));const o={},i="Image Captioning App",p={unversionedId:"imgcap/intro",id:"imgcap/intro",title:"Image Captioning App",description:"Now we are going to build an image captioning app using an open-source image-to-text",source:"@site/docs/imgcap/intro.md",sourceDirName:"imgcap",slug:"/imgcap/intro",permalink:"/doci-web/docs/imgcap/intro",draft:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/imgcap/intro.md",tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Streamlit",permalink:"/doci-web/docs/streamlit/intro"},next:{title:"Image Generation",permalink:"/doci-web/docs/imagegen/imagegenintro"}},l={},s=[],m={toc:s},c="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"image-captioning-app"},"Image Captioning App"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Now we are going to build an image captioning app using an open-source image-to-text")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"First will use our API key to set up and then we setup our helper file\nSource Key:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\nimport IPython.display\nfrom PIL import Image\nimport base64 \nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nhf_api_key = os.environ[\'HF_API_KEY\']\n#helperfunction\nimport requests, json\n#Image-to-text endpoint\ndef get_completion(inputs, parameters=None, \nENDPOINT_URL=os.environ[\'HF_API_ITT_BASE\']): \n headers = {\n "Authorization": f"Bearer {hf_api_key}",\n "Content-Type": "application/json"\n }\n data = { "inputs": inputs }\n if parameters is not None:\n data.update({"parameters": parameters})\n response = requests.request("POST",\n ENDPOINT_URL,\n headers=headers,\n data=json.dumps(data))\n return json.loads(response.content.decode("utf-8"))\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Here we also have have an image-to-text endpoint which is the endpoint\nfor the salesforce blip model(basically it is a model it is a model that\nreceives an image as an input and outputs the caption of said image.)"),(0,a.kt)("li",{parentName:"ul"},"And the way this works is that this model is trained on millions of those\nimages and text caption pairs in a way that they learn with the\nobjective of predicting what is the caption if it sees a new image.")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Building an Image Captioning App"),"\nIf you're running the model locally, you don't have to worry about that. But\nsince we're running it in the API format, we need to convert it to Base64 and\nback to be able to run this properly"),(0,a.kt)("p",null,"Here we'll be using an Inference Endpoint for Salesforce/blip-image-captioning-base a 14M parameter captioning model.\nThe code would look very similar if you were running it locally instead of from an API.\nYou can check the Pipelines documentation page."),(0,a.kt)("p",null,"Source Code:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import pipeline\nget_completion = pipeline("image-to-text",model="Salesforce/blip\n-image-captioning-base")\ndef summarize(input):\n output = get_completion(input)\n return output[0][\'generated_text\']\nThe free images are available on: https://free-images.com/\nSource Code for image importing \nimage_url = "https://free\x02images.com/sm/9596/dog_animal_greyhound_983023.jpg"\ndisplay(IPython.display.Image(url=image_url))\nget_completion(image_url)\nCaptioning with `gr.Interface()`\nimport gradio as gr \ndef image_to_base64_str(pil_image):\n byte_arr = io.BytesIO()\n pil_image.save(byte_arr, format=\'PNG\')\n byte_arr = byte_arr.getvalue()\n return str(base64.b64encode(byte_arr).decode(\'utf-8\'))\ndef captioner(image):\n base64_image = image_to_base64_str(image)\n result = get_completion(base64_image)\n return result[0][\'generated_text\']\ngr.close_all()\ndemo = gr.Interface(fn=captioner,\n inputs=[gr.Image(label="Upload image", type="pil")],\n outputs=[gr.Textbox(label="Caption")],\n title="Image Captioning with BLIP",\n description="Caption any image using the BLIP model",\n allow_flagging="never",\n examples=["christmas_dog.jpeg", "bird_flight.jpeg", "cow.jpeg"])\ndemo.launch(share=True, server_port=int(os.environ[\'PORT1\']))\n')),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("em",{parentName:"strong"},"This is how we build an image captioning app"))))}u.isMDXComponent=!0}}]);